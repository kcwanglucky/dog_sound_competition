{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "0c12802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "09ce373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from audio_classifier import *\n",
    "from config import path_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of samples: 1200\nwaveform shape: torch.Size([1, 40000])\nNumber of y\nNumber of label: 6\nLabel list: [0 1 2 3 4 5]\nNumber of remark: 11\nremark list: ['Barking' 'Howling' 'Crying' 'COSmoke' 'GlassBreaking' 'Other' 'Vacuum'\n 'Blender' 'Electrics' 'Cat' 'Dishes']\n"
     ]
    }
   ],
   "source": [
    "train_audio_path = path_config[\"train_audio_path\"]\n",
    "metadata = pd.read_csv(\"./train/meta_train.csv\")\n",
    "\n",
    "x_summary = explore_x(train_audio_path)\n",
    "y_summary = explore_y(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normal shape: torch.Size([1, 40000])\n",
      "\n",
      "Anomaly Shape Detected: train_01046.wav\n",
      "Shape: torch.Size([1, 16362])\n",
      "\n",
      "Total Anomaly: 1\n",
      "Before deleting anomaly: 1200 numbers of audios\n",
      "After deleting anomaly: 1199 numbers of audios\n",
      "Number of valid sample x: 1199\n",
      "Number of valid sample y: 1199\n"
     ]
    }
   ],
   "source": [
    "data = data_cleaning(train_audio_path, x_summary[\"waveform_shape\"], metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset = AudioDataset(train_audio_path, data[\"x_file_paths\"], data[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_split(audio_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n",
      "AudioClassifier(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (relu1): ReLU()\n",
      "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (pooling): AdaptiveAvgPool2d(output_size=1)\n",
      "  (linear): Linear(in_features=16, out_features=6, bias=True)\n",
      ")\n",
      "Epoch 1 Batch 1 Result\n",
      "\tLoss: 1.753\n",
      "Epoch 1 Batch 11 Result\n",
      "\tLoss: 1.809\n",
      "Epoch 1 Batch 21 Result\n",
      "\tLoss: 1.807\n",
      "Epoch: 1\n",
      "Train Stats:\n",
      "\tLoss: 1.81, Accuracy: 0.16\n",
      "Validation Stats:\n",
      "\tLoss: 0.06, Accuracy: 0.20, Auc: 0.52\n",
      "Epoch 2 Batch 1 Result\n",
      "\tLoss: 1.794\n",
      "Epoch 2 Batch 11 Result\n",
      "\tLoss: 1.793\n",
      "Epoch 2 Batch 21 Result\n",
      "\tLoss: 1.790\n",
      "Epoch: 2\n",
      "Train Stats:\n",
      "\tLoss: 1.79, Accuracy: 0.19\n",
      "Validation Stats:\n",
      "\tLoss: 0.06, Accuracy: 0.24, Auc: 0.61\n",
      "Epoch 3 Batch 1 Result\n",
      "\tLoss: 1.729\n",
      "Epoch 3 Batch 11 Result\n",
      "\tLoss: 1.760\n",
      "Epoch 3 Batch 21 Result\n",
      "\tLoss: 1.754\n",
      "Epoch: 3\n",
      "Train Stats:\n",
      "\tLoss: 1.74, Accuracy: 0.25\n",
      "Validation Stats:\n",
      "\tLoss: 0.06, Accuracy: 0.31, Auc: 0.69\n",
      "Epoch 4 Batch 1 Result\n",
      "\tLoss: 1.675\n",
      "Epoch 4 Batch 11 Result\n",
      "\tLoss: 1.690\n",
      "Epoch 4 Batch 21 Result\n",
      "\tLoss: 1.691\n",
      "Epoch: 4\n",
      "Train Stats:\n",
      "\tLoss: 1.69, Accuracy: 0.33\n",
      "Validation Stats:\n",
      "\tLoss: 0.05, Accuracy: 0.40, Auc: 0.77\n",
      "Epoch 5 Batch 1 Result\n",
      "\tLoss: 1.650\n",
      "Epoch 5 Batch 11 Result\n",
      "\tLoss: 1.629\n",
      "Epoch 5 Batch 21 Result\n",
      "\tLoss: 1.615\n",
      "Epoch: 5\n",
      "Train Stats:\n",
      "\tLoss: 1.61, Accuracy: 0.41\n",
      "Validation Stats:\n",
      "\tLoss: 0.05, Accuracy: 0.46, Auc: 0.83\n",
      "Epoch 6 Batch 1 Result\n",
      "\tLoss: 1.564\n",
      "Epoch 6 Batch 11 Result\n",
      "\tLoss: 1.552\n",
      "Epoch 6 Batch 21 Result\n",
      "\tLoss: 1.529\n",
      "Epoch: 6\n",
      "Train Stats:\n",
      "\tLoss: 1.51, Accuracy: 0.45\n",
      "Validation Stats:\n",
      "\tLoss: 0.05, Accuracy: 0.47, Auc: 0.86\n",
      "Epoch 7 Batch 1 Result\n",
      "\tLoss: 1.539\n",
      "Epoch 7 Batch 11 Result\n",
      "\tLoss: 1.467\n",
      "Epoch 7 Batch 21 Result\n",
      "\tLoss: 1.453\n",
      "Epoch: 7\n",
      "Train Stats:\n",
      "\tLoss: 1.45, Accuracy: 0.46\n",
      "Validation Stats:\n",
      "\tLoss: 0.05, Accuracy: 0.50, Auc: 0.88\n",
      "Epoch 8 Batch 1 Result\n",
      "\tLoss: 1.221\n",
      "Epoch 8 Batch 11 Result\n",
      "\tLoss: 1.381\n",
      "Epoch 8 Batch 21 Result\n",
      "\tLoss: 1.384\n",
      "Epoch: 8\n",
      "Train Stats:\n",
      "\tLoss: 1.38, Accuracy: 0.52\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.54, Auc: 0.89\n",
      "Epoch 9 Batch 1 Result\n",
      "\tLoss: 1.298\n",
      "Epoch 9 Batch 11 Result\n",
      "\tLoss: 1.369\n",
      "Epoch 9 Batch 21 Result\n",
      "\tLoss: 1.353\n",
      "Epoch: 9\n",
      "Train Stats:\n",
      "\tLoss: 1.33, Accuracy: 0.55\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.56, Auc: 0.90\n",
      "Epoch 10 Batch 1 Result\n",
      "\tLoss: 1.386\n",
      "Epoch 10 Batch 11 Result\n",
      "\tLoss: 1.300\n",
      "Epoch 10 Batch 21 Result\n",
      "\tLoss: 1.305\n",
      "Epoch: 10\n",
      "Train Stats:\n",
      "\tLoss: 1.29, Accuracy: 0.58\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.57, Auc: 0.90\n",
      "Epoch 11 Batch 1 Result\n",
      "\tLoss: 1.259\n",
      "Epoch 11 Batch 11 Result\n",
      "\tLoss: 1.301\n",
      "Epoch 11 Batch 21 Result\n",
      "\tLoss: 1.276\n",
      "Epoch: 11\n",
      "Train Stats:\n",
      "\tLoss: 1.26, Accuracy: 0.57\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.60, Auc: 0.90\n",
      "Epoch 12 Batch 1 Result\n",
      "\tLoss: 1.263\n",
      "Epoch 12 Batch 11 Result\n",
      "\tLoss: 1.209\n",
      "Epoch 12 Batch 21 Result\n",
      "\tLoss: 1.249\n",
      "Epoch: 12\n",
      "Train Stats:\n",
      "\tLoss: 1.23, Accuracy: 0.58\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.61, Auc: 0.90\n",
      "Epoch 13 Batch 1 Result\n",
      "\tLoss: 1.115\n",
      "Epoch 13 Batch 11 Result\n",
      "\tLoss: 1.204\n",
      "Epoch 13 Batch 21 Result\n",
      "\tLoss: 1.223\n",
      "Epoch: 13\n",
      "Train Stats:\n",
      "\tLoss: 1.22, Accuracy: 0.60\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.62, Auc: 0.90\n",
      "Epoch 14 Batch 1 Result\n",
      "\tLoss: 1.062\n",
      "Epoch 14 Batch 11 Result\n",
      "\tLoss: 1.197\n",
      "Epoch 14 Batch 21 Result\n",
      "\tLoss: 1.193\n",
      "Epoch: 14\n",
      "Train Stats:\n",
      "\tLoss: 1.20, Accuracy: 0.59\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.62, Auc: 0.90\n",
      "Epoch 15 Batch 1 Result\n",
      "\tLoss: 1.196\n",
      "Epoch 15 Batch 11 Result\n",
      "\tLoss: 1.200\n",
      "Epoch 15 Batch 21 Result\n",
      "\tLoss: 1.184\n",
      "Epoch: 15\n",
      "Train Stats:\n",
      "\tLoss: 1.20, Accuracy: 0.61\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.62, Auc: 0.90\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(data, len(y_summary[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3089e3c",
   "metadata": {},
   "source": [
    "### Resource:\n",
    "* scipy.io.wavfile.read\n",
    "* torchaudio\n",
    "* librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d120c38",
   "metadata": {},
   "source": [
    "### TODOs:\n",
    "* Data Augmentation \\\n",
    "    -> Random Time Shift \\\n",
    "    -> Time and Frequency Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "ab0c5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"./train/meta_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "9d3ed8b2",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Filename  Label   Remark\n",
       "0  train_00001      0  Barking\n",
       "1  train_00002      0  Barking\n",
       "2  train_00003      0  Barking\n",
       "3  train_00004      0  Barking\n",
       "4  train_00005      0  Barking"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Label</th>\n      <th>Remark</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_00001</td>\n      <td>0</td>\n      <td>Barking</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_00002</td>\n      <td>0</td>\n      <td>Barking</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_00003</td>\n      <td>0</td>\n      <td>Barking</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_00004</td>\n      <td>0</td>\n      <td>Barking</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_00005</td>\n      <td>0</td>\n      <td>Barking</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 452
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "027fd801",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = metadata[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "cbe64d9f",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "metadata": {},
     "execution_count": 454
    }
   ],
   "source": [
    "files = os.listdir(\"./train/train\")\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "90fed4a2",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of label: 6\nLabel list: [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "label = metadata[\"Label\"].unique()\n",
    "print(\"Number of label: {}\".format(len(label)))\n",
    "print(\"Label list: {}\".format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of remark: 11\nremark list: ['Barking' 'Howling' 'Crying' 'COSmoke' 'GlassBreaking' 'Other' 'Vacuum'\n 'Blender' 'Electrics' 'Cat' 'Dishes']\n"
     ]
    }
   ],
   "source": [
    "remark = metadata[\"Remark\"].unique()\n",
    "print(\"Number of remark: {}\".format(len(remark)))\n",
    "print(\"remark list: {}\".format(remark))"
   ]
  },
  {
   "source": [
    "Caution: Labels in the dataset is not a one-to-one mapping to remark. There are 11 types of remark in `Remark` column. Should use both `Label` and `Remark` column to do classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "9040c572",
   "metadata": {},
   "source": [
    "### Set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "81e60dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AUDIO_PATH = \"./train/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b452833",
   "metadata": {},
   "source": [
    "## Audio preprocessing + Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "5edff509",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    def read(audio_file):\n",
    "        \"\"\"Load an audio file. Return the signal as a tensor and the sample rate\"\"\"\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        return (waveform, sample_rate) #if waveform.shape == self.shape else None\n",
    "    \n",
    "    def spectro_gram(waveform, sample_rate, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        top_db = 80\n",
    "\n",
    "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "        spec = transforms.MelSpectrogram(sample_rate, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(waveform)\n",
    "\n",
    "        # Convert to decibels\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return (spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "2b7bd865",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = AudioProcessor.read(os.path.join(TRAIN_AUDIO_PATH, files[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "ce6c996a",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 40000])"
      ]
     },
     "metadata": {},
     "execution_count": 460
    }
   ],
   "source": [
    "waveform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94993d47",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "f2c82da7",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normal shape: torch.Size([1, 40000])\n",
      "\n",
      "Anomaly Shape Detected: train_01046.wav\n",
      "Shape: torch.Size([1, 16362])\n",
      "\n",
      "Total Anomaly: 1\n",
      "Before deleting anomaly: 1200 numbers of audios\n",
      "After deleting anomaly: 1199 numbers of audios\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"./train/train\")\n",
    "\n",
    "shape = waveform.shape\n",
    "print(\"Normal shape: {}\\n\".format(shape))\n",
    "\n",
    "num_files = len(files)\n",
    "to_remove = []\n",
    "count = 0\n",
    "for path in files:\n",
    "#     print(os.path.join(TRAIN_AUDIO_PATH, path))\n",
    "    waveform, sample_rate = audio_processor.read(os.path.join(TRAIN_AUDIO_PATH, path))\n",
    "    if waveform.shape != shape:\n",
    "        to_remove.append(path)\n",
    "        count += 1\n",
    "        print(\"Anomaly Shape Detected: {}\\nShape: {}\\n\".format(path, waveform.shape))\n",
    "\n",
    "for file in to_remove:\n",
    "    drop_idx = metadata[metadata[\"Filename\"] == file.split('.')[0]].index\n",
    "    metadata = metadata.drop(drop_idx)\n",
    "    files.remove(file)\n",
    "    \n",
    "print(\"Total Anomaly: {}\".format(count))\n",
    "print(\"Before deleting anomaly: {} numbers of audios\".format(num_files))\n",
    "print(\"After deleting anomaly: {} numbers of audios\".format(len(files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3f94f",
   "metadata": {},
   "source": [
    "##### Only one training data with different shape -> Just ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of valid sample x: 1199\nNumber of valid sample y: 1199\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of valid sample x: {}\".format(len(files)))\n",
    "print(\"Number of valid sample y: {}\".format(len(metadata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91271116",
   "metadata": {},
   "source": [
    "### Create a Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "e010a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, path_prefix, files, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.path_prefix = path_prefix\n",
    "        self.files = files\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.files[idx]\n",
    "        filepath = os.path.join(self.path_prefix, filename)\n",
    "        \n",
    "        waveform, sample_rate = AudioProcessor.read(filepath)\n",
    "        \n",
    "        specgram = AudioProcessor.spectro_gram(waveform, sample_rate, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        \n",
    "        y = self.y[self.y[\"Filename\"] == filename.split('.')[0]][\"Label\"].to_numpy()\n",
    "        \n",
    "        sample = {'x': specgram, 'y': y}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Filename, Label, Remark]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Label</th>\n      <th>Remark</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 464
    }
   ],
   "source": [
    "metadata[metadata[\"Filename\"] == \"train_01046\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "61648a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset = AudioDataset(TRAIN_AUDIO_PATH, files, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "7cafc930",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'x': tensor([[[-32.2916, -32.2916, -19.6115,  ..., -32.2916, -32.2916, -32.2916],\n",
       "          [-32.2916, -32.2916, -18.8911,  ..., -32.2916, -32.2916, -32.2916],\n",
       "          [-32.2916, -30.4037, -16.0444,  ..., -32.2916, -32.2916, -32.2916],\n",
       "          ...,\n",
       "          [-28.8417,  -8.9955,  -2.5214,  ..., -32.2916, -32.2916, -32.2916],\n",
       "          [-28.3178,  -8.9373,  -3.8161,  ..., -32.2916, -32.2916, -32.2916],\n",
       "          [-32.2916, -14.6754,  -6.4746,  ..., -32.2916, -32.2916, -32.2916]]]),\n",
       " 'y': array([1])}"
      ]
     },
     "metadata": {},
     "execution_count": 467
    }
   ],
   "source": [
    "audio_dataset[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = len(audio_dataset)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(audio_dataset, [num_train, num_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74ce83",
   "metadata": {},
   "source": [
    "### Create a Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=32,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataloader = DataLoader(val_ds, batch_size=32,\n",
    "                        shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "1bbc1a01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 torch.Size([32, 1, 64, 79]) torch.Size([32, 1])\n1 torch.Size([32, 1, 64, 79]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(audio_dataloader):\n",
    "    print(i_batch, sample_batched['x'].size(),\n",
    "          sample_batched['y'].size())\n",
    "    if i_batch == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Input Shape: torch.Size([32, 1, 64, 79])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Input Shape: {}\".format(sample_batched['x'].size()))"
   ]
  },
  {
   "source": [
    "### Build a CNN Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        # Linear Classifier\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.linear = nn.Linear(in_features=16, out_features=num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.pooling(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # Final output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "17aa4120",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AudioClassifier(\n  (conv1): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n  (relu1): ReLU()\n  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (relu2): ReLU()\n  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv): Sequential(\n    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n    (1): ReLU()\n    (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (4): ReLU()\n    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (pooling): AdaptiveAvgPool2d(output_size=1)\n  (linear): Linear(in_features=16, out_features=6, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# Create the model and put it on the GPU if available\n",
    "model = AudioClassifier(num_class = len(label))\n",
    "device = torch.device(device)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "105efdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_dataloader, val_dataloader, model, loss_fn, optimizer, num_epochs):\n",
    "    # Loss Function, Optimizer and Scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dataloader)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "    # Repeat for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "\n",
    "        # Repeat for each batch in the training set\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            x, y_true = data['x'].to(device), data['y'].to(device).reshape(-1)\n",
    "\n",
    "            # Normalize the inputs\n",
    "            mean, std = x.mean(), x.std()\n",
    "            x = (x - mean) / std\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y_true)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()   # Zero the parameter gradients\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (y_pred == y_true).sum().item()\n",
    "            total_prediction += y_pred.shape[0]\n",
    "\n",
    "            if i % 10 == 0:    # print every 10 mini-batches\n",
    "                oh_ytrue = torch.nn.functional.one_hot(y_true)\n",
    "                y_prob = torch.nn.functional.softmax(outputs, dim=1).detach().numpy()\n",
    "                auc = roc_auc_score(oh_ytrue, y_prob, multi_class=\"ovr\")\n",
    "                print(\"Epoch {} Batch {} Result\".format(epoch + 1, i + 1))\n",
    "                print('\\tLoss: %.3f' % (running_loss / (i + 1)))\n",
    "\n",
    "        val_acc, val_avg_loss, val_auc = validation_loop(val_dataloader, model, loss_fn)\n",
    "\n",
    "        # Print stats at the end of the epoch\n",
    "        num_batches = len(train_dataloader)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction/total_prediction\n",
    "        \n",
    "        print(\"Epoch: {}\".format(epoch + 1))\n",
    "        print(\"Train Stats:\")\n",
    "        print(f'\\tLoss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "        print(\"Validation Stats:\")\n",
    "        print(f'\\tLoss: {val_avg_loss:.2f}, Accuracy: {val_acc:.2f}, Auc: {val_auc:.2f}')\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(val_dataloader, model, loss_fn):\n",
    "    size = len(val_dataloader.dataset)\n",
    "\n",
    "    val_loss = 0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    y_prob_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader):\n",
    "            x, y_true = data['x'].to(device), data['y'].to(device).reshape(-1)\n",
    "\n",
    "            # Normalize the inputs\n",
    "            mean, std = x.mean(), x.std()\n",
    "            x = (x - mean) / std\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(x)\n",
    "            y_prob = torch.nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "            val_loss += loss_fn(outputs, y_true).item()\n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (y_pred == y_true).sum().item()\n",
    "            total_prediction += y_pred.shape[0]\n",
    "\n",
    "            y_true_all.append(y_true.cpu())\n",
    "            y_pred_all.append(y_pred.cpu())\n",
    "            y_prob_all.append(y_prob.cpu())\n",
    "\n",
    "    acc = correct_prediction/total_prediction\n",
    "    \n",
    "    # oh_ytrue = torch.nn.functional.one_hot(y_true)\n",
    "    # y_prob = torch.nn.functional.softmax(outputs, dim=1).detach().numpy()\n",
    "    # auc = roc_auc_score(oh_ytrue, y_prob, multi_class=\"ovr\")\n",
    "\n",
    "    # y_prob = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    y_true_all = torch.cat(y_true_all, dim = 0).detach().cpu().numpy()\n",
    "    y_pred_all = torch.cat(y_pred_all, dim = 0).detach().cpu().numpy()\n",
    "    y_prob_all = torch.cat(y_prob_all, dim = 0).detach().cpu().numpy()\n",
    "\n",
    "    auc = roc_auc_score(y_true_all, y_prob_all, multi_class=\"ovr\")\n",
    "\n",
    "    return acc, val_loss / size, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 Batch 1 Result\n",
      "\tLoss: 1.812\n",
      "Epoch 1 Batch 11 Result\n",
      "\tLoss: 1.787\n",
      "Epoch 1 Batch 21 Result\n",
      "\tLoss: 1.783\n",
      "Epoch: 1\n",
      "Train Stats:\n",
      "\tLoss: 1.79, Accuracy: 0.16\n",
      "Validation Stats:\n",
      "\tLoss: 0.06, Accuracy: 0.20, Auc: 0.60\n",
      "Epoch 2 Batch 1 Result\n",
      "\tLoss: 1.834\n",
      "Epoch 2 Batch 11 Result\n",
      "\tLoss: 1.767\n",
      "Epoch 2 Batch 21 Result\n",
      "\tLoss: 1.770\n",
      "Epoch: 2\n",
      "Train Stats:\n",
      "\tLoss: 1.76, Accuracy: 0.21\n",
      "Validation Stats:\n",
      "\tLoss: 0.06, Accuracy: 0.25, Auc: 0.66\n",
      "Epoch 3 Batch 1 Result\n",
      "\tLoss: 1.724\n",
      "Epoch 3 Batch 11 Result\n",
      "\tLoss: 1.735\n",
      "Epoch 3 Batch 21 Result\n",
      "\tLoss: 1.725\n",
      "Epoch: 3\n",
      "Train Stats:\n",
      "\tLoss: 1.71, Accuracy: 0.24\n",
      "Validation Stats:\n",
      "\tLoss: 0.06, Accuracy: 0.30, Auc: 0.72\n",
      "Epoch 4 Batch 1 Result\n",
      "\tLoss: 1.676\n",
      "Epoch 4 Batch 11 Result\n",
      "\tLoss: 1.650\n",
      "Epoch 4 Batch 21 Result\n",
      "\tLoss: 1.658\n",
      "Epoch: 4\n",
      "Train Stats:\n",
      "\tLoss: 1.65, Accuracy: 0.30\n",
      "Validation Stats:\n",
      "\tLoss: 0.05, Accuracy: 0.34, Auc: 0.77\n",
      "Epoch 5 Batch 1 Result\n",
      "\tLoss: 1.659\n",
      "Epoch 5 Batch 11 Result\n",
      "\tLoss: 1.617\n",
      "Epoch 5 Batch 21 Result\n",
      "\tLoss: 1.601\n",
      "Epoch: 5\n",
      "Train Stats:\n",
      "\tLoss: 1.58, Accuracy: 0.38\n",
      "Validation Stats:\n",
      "\tLoss: 0.05, Accuracy: 0.39, Auc: 0.81\n",
      "Epoch 6 Batch 1 Result\n",
      "\tLoss: 1.513\n",
      "Epoch 6 Batch 11 Result\n",
      "\tLoss: 1.487\n",
      "Epoch 6 Batch 21 Result\n",
      "\tLoss: 1.491\n",
      "Epoch: 6\n",
      "Train Stats:\n",
      "\tLoss: 1.50, Accuracy: 0.48\n",
      "Validation Stats:\n",
      "\tLoss: 0.05, Accuracy: 0.47, Auc: 0.84\n",
      "Epoch 7 Batch 1 Result\n",
      "\tLoss: 1.360\n",
      "Epoch 7 Batch 11 Result\n",
      "\tLoss: 1.434\n",
      "Epoch 7 Batch 21 Result\n",
      "\tLoss: 1.440\n",
      "Epoch: 7\n",
      "Train Stats:\n",
      "\tLoss: 1.43, Accuracy: 0.52\n",
      "Validation Stats:\n",
      "\tLoss: 0.05, Accuracy: 0.51, Auc: 0.86\n",
      "Epoch 8 Batch 1 Result\n",
      "\tLoss: 1.332\n",
      "Epoch 8 Batch 11 Result\n",
      "\tLoss: 1.403\n",
      "Epoch 8 Batch 21 Result\n",
      "\tLoss: 1.391\n",
      "Epoch: 8\n",
      "Train Stats:\n",
      "\tLoss: 1.38, Accuracy: 0.55\n",
      "Validation Stats:\n",
      "\tLoss: 0.05, Accuracy: 0.53, Auc: 0.86\n",
      "Epoch 9 Batch 1 Result\n",
      "\tLoss: 1.277\n",
      "Epoch 9 Batch 11 Result\n",
      "\tLoss: 1.353\n",
      "Epoch 9 Batch 21 Result\n",
      "\tLoss: 1.334\n",
      "Epoch: 9\n",
      "Train Stats:\n",
      "\tLoss: 1.33, Accuracy: 0.57\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.56, Auc: 0.87\n",
      "Epoch 10 Batch 1 Result\n",
      "\tLoss: 1.311\n",
      "Epoch 10 Batch 11 Result\n",
      "\tLoss: 1.319\n",
      "Epoch 10 Batch 21 Result\n",
      "\tLoss: 1.298\n",
      "Epoch: 10\n",
      "Train Stats:\n",
      "\tLoss: 1.29, Accuracy: 0.59\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.56, Auc: 0.88\n",
      "Epoch 11 Batch 1 Result\n",
      "\tLoss: 1.209\n",
      "Epoch 11 Batch 11 Result\n",
      "\tLoss: 1.305\n",
      "Epoch 11 Batch 21 Result\n",
      "\tLoss: 1.293\n",
      "Epoch: 11\n",
      "Train Stats:\n",
      "\tLoss: 1.27, Accuracy: 0.59\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.56, Auc: 0.88\n",
      "Epoch 12 Batch 1 Result\n",
      "\tLoss: 1.264\n",
      "Epoch 12 Batch 11 Result\n",
      "\tLoss: 1.263\n",
      "Epoch 12 Batch 21 Result\n",
      "\tLoss: 1.255\n",
      "Epoch: 12\n",
      "Train Stats:\n",
      "\tLoss: 1.25, Accuracy: 0.61\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.57, Auc: 0.88\n",
      "Epoch 13 Batch 1 Result\n",
      "\tLoss: 1.224\n",
      "Epoch 13 Batch 11 Result\n",
      "\tLoss: 1.202\n",
      "Epoch 13 Batch 21 Result\n",
      "\tLoss: 1.230\n",
      "Epoch: 13\n",
      "Train Stats:\n",
      "\tLoss: 1.22, Accuracy: 0.64\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.57, Auc: 0.88\n",
      "Epoch 14 Batch 1 Result\n",
      "\tLoss: 1.282\n",
      "Epoch 14 Batch 11 Result\n",
      "\tLoss: 1.232\n",
      "Epoch 14 Batch 21 Result\n",
      "\tLoss: 1.217\n",
      "Epoch: 14\n",
      "Train Stats:\n",
      "\tLoss: 1.22, Accuracy: 0.62\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.56, Auc: 0.88\n",
      "Epoch 15 Batch 1 Result\n",
      "\tLoss: 1.141\n",
      "Epoch 15 Batch 11 Result\n",
      "\tLoss: 1.227\n",
      "Epoch 15 Batch 21 Result\n",
      "\tLoss: 1.225\n",
      "Epoch: 15\n",
      "Train Stats:\n",
      "\tLoss: 1.21, Accuracy: 0.64\n",
      "Validation Stats:\n",
      "\tLoss: 0.04, Accuracy: 0.57, Auc: 0.88\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "loss_fnc = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 15\n",
    "\n",
    "training(train_dataloader, val_dataloader, model, loss_fnc, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataloader[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1200\n"
     ]
    }
   ],
   "source": [
    "print(len(audio_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python394jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.9.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}